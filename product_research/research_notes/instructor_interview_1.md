## Interview Snapshot
- **Interview date:** 2025-10-07
- **Interviewer(s):** Samuel Ruo
- **Participant name / role:** Anonymous CS Professor
- **Institution / program:** University of Toronto

## Summary of Interview
- While attending an office hour I was able to have an informal chat with a CS professor at UofT. This conversation had surprising relevance to our work and I was able to ask them some follow-up questions to get more context. I am recording these notes here for future reference to our project work and goals. I am keeping the professor anonymous as this was an informal chat and not a formal interview.
- The main insight I gained from this is how the results of a previous cohort affected how the professor decided to structure the course for the current cohort. The reasons for these changes are likely due to AI/LLM use in the class.

## Participant Context
- Course(s) taught / responsibilities: Upper-year undergraduate CS course; responsible for course design and delivery and has taught this course for many years.
- Class size and modality: ~100+ students, in person
- Tool stack currently in use: Markus, Crowdmark, Piazza

## Highlights
- Major themes observed:
  - Classes are scoring incredibly high for assignments and hand-in work; however, performance on tests is significantly lower.
  - This has led them to be concerned that the students are not actually learning the material.
  - This has led them to change the structure of the course to have more in-person assessments and less take-home work in an effort to ensure students are actually learning the material.
  
## Problems & Pain Points
- In the last year the professor saw a significant increase in assignment scores; however, when questions on tests related to the assignment material were introduced, students were not able to answer them as well as they had on the assignments. According to the professor, these questions were not tricky, and if students had properly done the assignments they would have been able to answer them very easily.
- This concerned them as they felt that students were not actually learning the material, but rather could be using AI tools to get answers for their assignments.
- This has led them to change the structure of the course to have more in-person assessments making up a larger portion of the final grade, and less graded take-home work.
  - They understand that this is not a perfect solution and could have some negative consequences for students, but they felt it was necessary to ensure students were actually learning the material.
  - This is also more work for them and the teaching team as they had to create more in person assessments (and variations of them for different class sections), which also means more grading work for them.
- Outside of learning, this also causes them issues for the reputation of their program/class. They mentioned how some previous students of the course had done very well and that certain companies contact the professor looking for strong students from the class. If students are not actually learning the material, this could lead to a decrease in the reputation of their program/class, leading to fewer of these opportunities for students in the future.

## Needs, Opportunities, & Areas for Impact
- Confirmed that we may be on the right track with our tool to help instructors ensure students are actually learning the material.
- Not necessarily bad that students are using AI tools to get answers, but they need to be learning the material and not just getting answers.
  - The professor did not seem to be worried about the fact that students were scoring high on assignments, but rather the discrepancy between assignment scores and test scores, which was seen as a sign that students were not actually learning the material.
  - In other words, for this professor in particular, there did not seem to be a big issue with academic integrity or cheating, but rather ensuring students are actually learning the material. (Our tool should focus on this aspect, which at the moment is our current focus.)
- Our tool could help reduce the need for instructors to change their course structure to have more in-person assessments, which is more work for them and less flexible for students.
- In addressing these issues we can ensure that instructors are still training strong students, upholding the reputation of their program/class.
  
## Next Steps
- Have instructor testing of the student interface, not just the instructor interface.
- In the long term consider testing our tool versus other AI tools in a full course setting to see if it truly resolves these issues.
- Decision date or milestone tied to this interview: TBD
  - This information may change how we prioritize the existing CUJ features we are working on, in particular on the instructor side. We will also be discussing how we can ensure the student side of our tool is easy and effective in addressing these issues.
